---
categories:
- NLP
- "\u5BF9\u8BDD"
comment: true
date: 2023-01-18 05:28:40+08:00
draft: false
featuredimagepreview: https://blog.hzxwhzxw.asia/featured-image/8.webp
hiddenFromHomePage: false
hiddenFromSearch: false
lightgallery: true
math:
  enable: true
repost:
  enable: false
  url: ""
seo:
  images: []
tags:
- "\u591A\u8F6E\u5BF9\u8BDD"
title: "\u751F\u6210\u591A\u8F6E\u5BF9\u8BDD\u5B9E\u8DF5\uFF08\u4E00\uFF09"
toc:
  enable: true
weight: 0
---

## 语料简介

LCCC数据集（Large-scale Cleaned Chinese Conversation）是一个大规模的中文对话数据集，由研究者提供并在Github上公开发布。该数据集包含了两个版本，分别为LCCC-base和LCCC-large。

LCCC-base版本主要来源于微博对话，其中包含了3,354,382轮单轮对话和3,466,607轮多轮对话，总共6,708,554个对话语句。而LCCC-large版本在LCCC-base的基础上进行了融合，并加入了其他开源对话语料，其中包含了7,273,804轮单轮对话和4,733,955轮多轮对话，总共14,547,608个对话语句。具体的数据如下表所示：

| LCCC版本 | 单轮对话轮次 | 多轮对话轮次 | 总对话语句 |
| -------- | ------------ | ------------ | ---------- |
| base     | 3,354,382    | 3,466,607    | 6,708,554  |
| large    | 7,273,804    | 4,733,955    | 14,547,608 |

LCCC数据集经过了严格的清洗过程，其中包括了一系列手工规则和基于机器学习算法构建的分类器，来确保对话数据的质量。在清洗过程中，过滤了脏字脏词、特殊字符、颜表情、语法不通的语句、上下文不相关的对话等噪声。

该数据集已经被广泛使用，用于训练多轮对话模型，并在相关领域取得了良好的研究成果。

## 数据格式

为了简化任务，所有样本都被处理成双人对话。下面是一些样本示例：

>A: 等过年咱们回去买点兔头好好吃顿火锅
>
>B: 太原就没看见有好吃的兔头
>
>A: 我从虹桥给你带个回去那天瞅到一正宗的
>
>B: 最爱你了
>
>A: 那是必须
>
>A: 嗯嗯，我再等等！你现在在上海吧？上海风好像比南京还大呢，少出门吧
>
>B: 对啊，我在家，没事儿。一定要小心啊！
>
>A: 我去年也去转了一圈，还碰见以前的体育老师了，合了个影
>
>B: 哈哈我还去找高一时侯的英语老师没找到她刚好有事情没在学校～
>
>A: 你也是真心找回忆了哦
>
>B: 哈哈毕业了没去过想去看看啊

## 模型设计

在了解数据特征后，我们需要进行模型设计。显然，我们的目的是训练一个模型来预测下一个应该回复的内容。由于语料中包含了多轮对话，因此我们还需要要求这个模型能够支持多轮对话。

考虑到对话历史的最简单方式是将直到当前句的所有历史对话拼接成单句文本作为模型的输入信息。从形式上来看，我们应该使用Seq2Seq模型来完成这个任务。然而，直接使用Seq2Seq模型可能会有一些问题。标准的Seq2Seq模型通常用于形式比较固定的输入输出，如输入文本长度应该集中在某个范围内，不宜变化太大。考虑到多轮对话，理论上我们不知道前面有多少轮对话，因此原则上输入文本长度是无限制的。此外，使用Seq2Seq模型还存在训练效率低的问题。

因此，我们需要一个长度能相当自由地变化的、同时能预测整一个多轮对话的模型。实现这个需求的比较适当的选择就是单向语言模型（LM、GPT），做法如下图：

[![利用单向语言模型做多轮对话示意图](https://image-1304916025.cos.ap-nanjing.myqcloud.com/typora-603395458.png)](https://kexue.fm/usr/uploads/2020/09/603395458.png)

如图所示，我们选择当前主流的Transformer模型，按照BERT的常规输入格式，将每句对话用[SEP]拼接起来，然后就训练一个从左往右的单向语言模型。为了区分不同的说话角色，我们对不同的说话者用不同的Segment Id区分。此外，考虑到BERT和GPT都是用了绝对位置编码，可处理的文本长度存在一个上限，而对话轮数理论上是无限的，所以这里我们采用了相对位置编码的NEZHA作为基本结构，并使用NEZHA的预训练权重作为模型的初始化权重。

说白了，就是往NEZHA里边加入了下三角形式的Attention Mask，使其变为一个语言模型，相关介绍请参考[《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》](https://kexue.fm/archives/6933)。

## 训练细节

首先，给出笔者的参考实现以及训练好的模型：

> https://github.com/bojone/nezha_gpt_dialog

代码基于bert4keras实现，基本结构是NEZHA Base。为了使得训练更加“平民化”，笔者只用了一张卡（22G的RTX）配合梯度累积来训练，显存不够大的读者可以缩小batch_size。开源的模型是用2×10−52×10−5的学习率、0.010.01的权重衰减率的Adam优化器进行训练的，训练了1000万步（20多天），单步batch_size为16，累积梯度步数为16，所以相当于以256的batch_size训练了62.5万步，最终的loss大概是2.83（平均每token的交叉熵）。

我们来测试一下我们的训练结果。

```python
>>> print(chatbot.response([u'我最近天天吃火锅']))
火锅好吃啊
>>> print(chatbot.response([u'我最近天天吃火锅']))
我也是
>>> print(chatbot.response([u'我最近天天吃火锅']))
哈哈哈哈哈哈我最近一周吃了两顿火锅
>>> print(chatbot.response([u'我最近天天吃火锅']))
我也是，每次都吃的撑
>>> print(chatbot.response([u'我最近天天吃火锅']))
吃火锅不长胖
>>> print(chatbot.response([u'我最近天天吃火锅']))
火锅好啊
>>> print(chatbot.response([u'我最近天天吃火锅']))
你们那边有这种火锅？
>>> print(chatbot.response([u'我最近天天吃火锅']))
火锅好吃吗？我也想吃
>>> print(chatbot.response([u'我最近天天吃火锅']))
吃吧，吃了就开心了
>>> print(chatbot.response([u'我最近天天吃火锅']))
我也想天天吃火锅！
>>> print(chatbot.response([u'我最近天天吃火锅']))
火锅吃不了多久的。
>>> print(chatbot.response([u'你那天气好吗']))
还好，不热，有点冷
>>> print(chatbot.response([u'你那天气好吗', u'还好，不热，有点冷', u'那你得穿多厚的衣服']))
我穿了两件
```

## 对比分析

CDial-GPT也开源了自己训练的预训练模型，笔者也将它转换为bert4keras能加载的格式了（[CDial-GPT-tf](https://github.com/bojone/CDial-GPT-tf)），读者也可以测试比对一下。从训练上来看，CDial-GPT使用pytorch实现的模型，基本结构是GPT Base，使用了4张2080Ti，总batch_size为32，累积梯度64步，论文说训练了30个epoch，总步数约2100万步（笔者的两倍），因此大概相当于batch_size为2048训练了33万步。

在输入设计上，CDial-GPT也有所不同，如下图：

[![CDial-GPT模型示意图](https://image-1304916025.cos.ap-nanjing.myqcloud.com/typora-727199313.png)](https://kexue.fm/usr/uploads/2020/09/727199313.png)

如图所示，CDial-GPT跟我们前述设计的主要不同是多轮对话之间的拼接方式，我们之前是直接用[SEP]连接，它是用[speaker1]、[speaker2]（图中简记为S1、S2）这样的角色标记来连接，最后才用一个[SEP]表示回复结束。这样一来，由于预测部分的格式跟历史的格式不一样，因此每次只能训练一句回复，多轮对话要拆分为多个样本来训练，理论上是增加了训练复杂性的（要训练多步才能把一个多轮对话样本训练完）。

至于效果上，个人测试的感觉是两者没什么明显差别。有兴趣的读者也可以自行比较测试。

## 文章总结

本文主要分享了一次对话模型实践，基于CDial-GPT开源的LCCC闲聊语料库，利用语言模型（GPT）对多轮对话进行生成式建模，得到了一个相对通用的闲聊对话模型，最后将本文的思路与CDial-GPT本身开源的模型进行了比较。

## 参考文献

[1] [CDial-GPT: A Large-Scale Pretrained Language Model for Chinese Conversation Generation](https://arxiv.org/abs/2009.00300)
