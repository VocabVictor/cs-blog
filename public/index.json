[{"categories":["AI工具"],"content":"Anaconda安装 ","date":"2023-01-29","objectID":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/:1:0","tags":["AI工具"],"title":"Anaconda使用教程（一）","uri":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/"},{"categories":["AI工具"],"content":"Anaconda简介 Anaconda包括Conda、Python以及一大堆安装好的工具包，比如：numpy、pandas等 因此安装Anaconda的好处主要为以下几点： 1）包含conda：conda是一个环境管理器，其功能依靠conda包来实现，该环境管理器与pip类似，那有童鞋会问了：我能通过pip装conda包达到conda环境管理器一样的功能吗？答案是不能，conda包的实现离不开conda环境管理器。想详细知道两者异同可以去知乎遛一遛https://www.zhihu.com/question/279152320 2）安装大量工具包：Anaconda会自动安装一个基本的python，该python的版本Anaconda的版本有关。该python下已经装好了一大堆工具包，这对于科学分析计算是一大便利，你愿意费时耗力使用pip一个个包去装吗？ 3）可以创建使用和管理多个不同的Python版本：比如想要新建一个新框架或者使用不同于Anoconda装的基本Python版本，Anoconda就可以实现同时多个python版本的管理 ","date":"2023-01-29","objectID":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/:1:1","tags":["AI工具"],"title":"Anaconda使用教程（一）","uri":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/"},{"categories":["AI工具"],"content":"Anaconda安装情况的选择 Anaconda的安装分两种情况： 情况一：电脑现在没有装python或者现在装的可以卸载掉（装Anaconda时先卸python）； 情况二：电脑目前装了python，但想保留它； 情况一 Anaconda的下载 你可以根据你的操作系统是32位还是64位选择对应的版本到官网下载，但是官网下载龟速，建议到清华大学镜像站下载，多快又好省，博主使用的版本是： Anaconda3-5.2.0-Windows-x86_64.exe 为什么不用最新版的 Anaconda3-5.3.1-Windows-x86_64.exe 不知是版本原因还是什么原因，包括博主在内的一大堆使用这个最新版本在构建虚拟环境或者安装包时出现了这样蛋疼的错误 无法定位程序输入点 OPENSSL_sk_new_reserve 于动态链接库 E：\\ProgramData\\Anaconda3\\Library\\bin\\libssl-1_1-x64.dll上 最后有博文指出回退3-5.2.0版本毛事木有 下载好Anaconda3后直接双击安装包即可，有几个地方需要注意 Finish后安装完毕 2.1.2 测试安装 cmd输入 conda –version 若出现像这样的conda版本号即安装成功 2.1.3 更改源 使用 conda install 包名 安装需要的Python包非常方便，但是官方服务器在国外，下载龟速，国内清华大学提供了Anaconda的镜像仓库，我们把源改为清华大学镜像源 更改方法一：cmd后依次输入下面命令 conda config –add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/ conda config –set show_channel_urls yes 打开C盘用户目录，我这里是 C:\\Users\\User 找到.condarc文件，里面长这样就成了 ssl_verify: true channels: https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ defaults show_channel_urls: true 更改方法二：打开 .condarc文件，直接简单粗暴的把上面的内容复制进去 2.1.4 更新包 更新时间较长，建议找个空余时间更新,不更新也可以，但为避免后续安装其他东西出错最好更一下，这里我就不更了，把命令贴出来 先更新conda conda update conda 再更新第三方所有包 conda upgrade –all 2.1.5 创建和管理虚拟环境 第一种情况下Anaconda的安装到这里基本就结束了，后面关于虚拟环境部分属于Anaconda的使用了，这里会在第二种情况下再做介绍 2.2 情况二 情况二Anaconda的安装和情况一相同，但为保留自己安装的Python需要在安装Anaconda完成后进行操作 进行操作有2个方法，这里更加推荐方法二 2.2.1 方法一：通过更改python.exe文件名 Anaconda安装时会自带一个Python，没装之前我们先看看电脑里Python的版本（姑且称为原生python），cmd后输入： python –version 或者 python -V 这里显示原生python版本是3.7.4，我们到环境变量去看是这样的 C:\\ProgramFiles\\Python37\\Scripts; C:\\ProgramFiles\\Python37; %SystemRoot%\\system32; %SystemRoot%; %SystemRoot%\\System32\\Wbem; %SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0; C:\\Program Files\\PuTTY 按照第一种情况安装Anoconda后再输入 python –version 这时显示的是3.6.5，这里的版本就是Anaconda自带的python版本，我们再打开环境变量 C:\\ProgramData\\Anaconda3; C:\\ProgramData\\Anaconda3\\Library\\mingww64\\bin; C:\\ProgramData\\Anaconda3\\Library\\usr\\bin; C:\\ProgramData\\Anaconda3\\Library\\bin; C:\\ProgramData\\Anaconda3\\Scripts; C:\\ProgramFiles\\Python37\\Scripts; C:\\ProgramFiles\\Python37; %SystemRoot%\\system32; %SystemRoot%; %SystemRoot%\\System32\\Wbem; %SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0; C:\\Program Files\\PuTTY 发现原生Python路径还在，同时在原生Python路径之前多了与Anaconda相关的路径，因此Anaconda自带安装的Python并不会覆盖掉原生Python，但为什么输python –version显示的是Anaconda的版本而不是原生的呢？这是因为环境变量优先级的缘故，这里Anaconda在前，原生在后，更改他们的顺序后输入python –version可以得到原生的版本号，有兴趣的童鞋可以自己尝试。 C:\\Program Files\\Python37; C:\\ProgramFiles\\Python37\\Scripts; C:\\ProgramData\\Anaconda3;C:\\ProgramData\\Anaconda3\\Library\\mingww64\\bin; C:\\ProgramData\\Anaconda3\\Library\\usr\\bin; C:\\ProgramData\\Anaconda3\\Library\\bin; C:\\ProgramData\\Anaconda3\\Scripts; %SystemRoot%\\system32; %SystemRoot%; %SystemRoot%\\System32\\Wbem; %SYSTEMROOT%\\System32\\WindowsPowerShell\\v1.0; C:\\Program Files\\PuTTY\\ 因此方法一来了，把原生python安装路径下的python.exe改为python_ori.exe 再把Anaconda安装路径下的python.exe改为python_ana.exe 查看版本： 使用时要注意区分，如进行pip安装时 python_ori –m pip install 包名 python_ano –m pip install 包名 2.2.2 方法二：通过切换虚拟环境 输入 conda info -e 或者 conda-env list 查看Anaconda中当前存在的环境 可以看到当前只存在一个叫做base的环境，这个环境即是Anaconda安装的Python版本 Anaconda装的版本是3.6.5的，假如我们想使用2.7版本的，这时可以通过创建虚拟环境来实现，输入 conda create -n python27 python=2.7 不用管是输入2.7.x，还是2.7，conda会为我们自动寻找2.7.x中的最新版本，再次查看Anaconda中存在的环境 发现较之前多了一个python27，我们到Anaconda安装目录查看envs文件夹下的python27 点进去看发现这不就是一个python安装过后的文件吗，说是创建虚拟环境，其实是真实的安装了Python2.7，我们切换至2.7版本的，输入 activate python27 切换成功后前面多一个python27 这时我们保留原生python就有了思路： 1）在Anaconda安装目录下的envi文件内新建一个名为python_ori的文件（没有envs文件夹就自己新建） 2）将原生python整个安装目录复制python_ori 3）全部复制后粘贴到python_ori 4）cmd后激活切换至原生的python 查询版本号 没问题，3.7.4是原生版本，那是那个味哈哈。 ","date":"2023-01-29","objectID":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/:1:2","tags":["AI工具"],"title":"Anaconda使用教程（一）","uri":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/"},{"categories":["AI工具"],"content":"Anacoda环境创建","date":"2023-01-29","objectID":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/:2:0","tags":["AI工具"],"title":"Anaconda使用教程（一）","uri":"/anaconda%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B%E4%B8%80/"},{"categories":["NLP","对话"],"content":"语料简介 LCCC数据集（Large-scale Cleaned Chinese Conversation）是一个大规模的中文对话数据集，由研究者提供并在Github上公开发布。该数据集包含了两个版本，分别为LCCC-base和LCCC-large。 LCCC-base版本主要来源于微博对话，其中包含了3,354,382轮单轮对话和3,466,607轮多轮对话，总共6,708,554个对话语句。而LCCC-large版本在LCCC-base的基础上进行了融合，并加入了其他开源对话语料，其中包含了7,273,804轮单轮对话和4,733,955轮多轮对话，总共14,547,608个对话语句。具体的数据如下表所示： LCCC版本 单轮对话轮次 多轮对话轮次 总对话语句 base 3,354,382 3,466,607 6,708,554 large 7,273,804 4,733,955 14,547,608 LCCC数据集经过了严格的清洗过程，其中包括了一系列手工规则和基于机器学习算法构建的分类器，来确保对话数据的质量。在清洗过程中，过滤了脏字脏词、特殊字符、颜表情、语法不通的语句、上下文不相关的对话等噪声。 该数据集已经被广泛使用，用于训练多轮对话模型，并在相关领域取得了良好的研究成果。 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:1:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"数据格式 为了简化任务，所有样本都被处理成双人对话。下面是一些样本示例： A: 等过年咱们回去买点兔头好好吃顿火锅 B: 太原就没看见有好吃的兔头 A: 我从虹桥给你带个回去那天瞅到一正宗的 B: 最爱你了 A: 那是必须 A: 嗯嗯，我再等等！你现在在上海吧？上海风好像比南京还大呢，少出门吧 B: 对啊，我在家，没事儿。一定要小心啊！ A: 我去年也去转了一圈，还碰见以前的体育老师了，合了个影 B: 哈哈我还去找高一时侯的英语老师没找到她刚好有事情没在学校～ A: 你也是真心找回忆了哦 B: 哈哈毕业了没去过想去看看啊 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:2:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"模型设计 在了解数据特征后，我们需要进行模型设计。显然，我们的目的是训练一个模型来预测下一个应该回复的内容。由于语料中包含了多轮对话，因此我们还需要要求这个模型能够支持多轮对话。 考虑到对话历史的最简单方式是将直到当前句的所有历史对话拼接成单句文本作为模型的输入信息。从形式上来看，我们应该使用Seq2Seq模型来完成这个任务。然而，直接使用Seq2Seq模型可能会有一些问题。标准的Seq2Seq模型通常用于形式比较固定的输入输出，如输入文本长度应该集中在某个范围内，不宜变化太大。考虑到多轮对话，理论上我们不知道前面有多少轮对话，因此原则上输入文本长度是无限制的。此外，使用Seq2Seq模型还存在训练效率低的问题。 因此，我们需要一个长度能相当自由地变化的、同时能预测整一个多轮对话的模型。实现这个需求的比较适当的选择就是单向语言模型（LM、GPT），做法如下图： 如图所示，我们选择当前主流的Transformer模型，按照BERT的常规输入格式，将每句对话用[SEP]拼接起来，然后就训练一个从左往右的单向语言模型。为了区分不同的说话角色，我们对不同的说话者用不同的Segment Id区分。此外，考虑到BERT和GPT都是用了绝对位置编码，可处理的文本长度存在一个上限，而对话轮数理论上是无限的，所以这里我们采用了相对位置编码的NEZHA作为基本结构，并使用NEZHA的预训练权重作为模型的初始化权重。 说白了，就是往NEZHA里边加入了下三角形式的Attention Mask，使其变为一个语言模型，相关介绍请参考《从语言模型到Seq2Seq：Transformer如戏，全靠Mask》。 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:3:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"训练细节 首先，给出笔者的参考实现以及训练好的模型： https://github.com/bojone/nezha_gpt_dialog 代码基于bert4keras实现，基本结构是NEZHA Base。为了使得训练更加“平民化”，笔者只用了一张卡（22G的RTX）配合梯度累积来训练，显存不够大的读者可以缩小batch_size。开源的模型是用2×10−52×10−5的学习率、0.010.01的权重衰减率的Adam优化器进行训练的，训练了1000万步（20多天），单步batch_size为16，累积梯度步数为16，所以相当于以256的batch_size训练了62.5万步，最终的loss大概是2.83（平均每token的交叉熵）。 我们来测试一下我们的训练结果。 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 火锅好吃啊 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 我也是 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 哈哈哈哈哈哈我最近一周吃了两顿火锅 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 我也是，每次都吃的撑 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 吃火锅不长胖 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 火锅好啊 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 你们那边有这种火锅？ \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 火锅好吃吗？我也想吃 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 吃吧，吃了就开心了 \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 我也想天天吃火锅！ \u003e\u003e\u003e print(chatbot.response([u'我最近天天吃火锅'])) 火锅吃不了多久的。 \u003e\u003e\u003e print(chatbot.response([u'你那天气好吗'])) 还好，不热，有点冷 \u003e\u003e\u003e print(chatbot.response([u'你那天气好吗', u'还好，不热，有点冷', u'那你得穿多厚的衣服'])) 我穿了两件 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:4:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"对比分析 CDial-GPT也开源了自己训练的预训练模型，笔者也将它转换为bert4keras能加载的格式了（CDial-GPT-tf），读者也可以测试比对一下。从训练上来看，CDial-GPT使用pytorch实现的模型，基本结构是GPT Base，使用了4张2080Ti，总batch_size为32，累积梯度64步，论文说训练了30个epoch，总步数约2100万步（笔者的两倍），因此大概相当于batch_size为2048训练了33万步。 在输入设计上，CDial-GPT也有所不同，如下图： 如图所示，CDial-GPT跟我们前述设计的主要不同是多轮对话之间的拼接方式，我们之前是直接用[SEP]连接，它是用[speaker1]、[speaker2]（图中简记为S1、S2）这样的角色标记来连接，最后才用一个[SEP]表示回复结束。这样一来，由于预测部分的格式跟历史的格式不一样，因此每次只能训练一句回复，多轮对话要拆分为多个样本来训练，理论上是增加了训练复杂性的（要训练多步才能把一个多轮对话样本训练完）。 至于效果上，个人测试的感觉是两者没什么明显差别。有兴趣的读者也可以自行比较测试。 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:5:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"文章总结 本文主要分享了一次对话模型实践，基于CDial-GPT开源的LCCC闲聊语料库，利用语言模型（GPT）对多轮对话进行生成式建模，得到了一个相对通用的闲聊对话模型，最后将本文的思路与CDial-GPT本身开源的模型进行了比较。 ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:6:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"},{"categories":["NLP","对话"],"content":"参考文献 [1] CDial-GPT: A Large-Scale Pretrained Language Model for Chinese Conversation Generation ","date":"2023-01-18","objectID":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/:7:0","tags":["多轮对话"],"title":"生成多轮对话实践（一）","uri":"/%E7%94%9F%E6%88%90%E5%A4%9A%E8%BD%AE%E5%AF%B9%E8%AF%9D%E5%AE%9E%E8%B7%B5%E4%B8%80/"}]